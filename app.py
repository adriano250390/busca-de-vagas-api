from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import httpx
import redis
import os
import asyncio

# üîπ Novo: importa√ß√µes para o jobspy e CSV
import csv
from jobspy import scrape_jobs

app = FastAPI()

# Configura√ß√£o do Redis (Cache)
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379")
cache = redis.from_url(REDIS_URL, decode_responses=True)

# Configura√ß√£o da API Jooble (COMENTADA PARA TESTES)
JOOBLE_API_KEY = "814146c8-68bb-45cd-acd7-cd907162dc28"
JOOBLE_API_URL = "https://br.jooble.org/api/"

# Habilitar CORS corretamente
app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://gray-termite-250383.hostingersite.com"],
    allow_credentials=True,
    allow_methods=["GET", "POST"],
    allow_headers=["*"],
)

@app.get("/")
def home():
    """Rota principal da API"""
    return {"message": "API de busca de vagas est√° rodando!"}

@app.get("/healthz")
@app.head("/healthz")  # üîπ Suporte para requisi√ß√µes HEAD (necess√°rio para o UptimeRobot)
def health_check():
    """Rota de Health Check para o Render e monitoramento"""
    return {"status": "ok"}

# ----------------------------------------------------------------------------
# Rota /buscar que, por enquanto, usa APENAS o jobspy (Jooble COMENTADO)
# ----------------------------------------------------------------------------
@app.get("/buscar")
def buscar_vagas(termo: str, localizacao: str = "", pagina: int = 1):
    """
    Rota que busca vagas usando a biblioteca 'jobspy'.
    O c√≥digo do Jooble est√° comentado para testes de configura√ß√£o.
    Assim que estiver tudo certo, pode-se descomentar o Jooble e unir ambos.
    """

    # =========================================================================
    #                            (JOOBLE COMENTADO)
    # =========================================================================
    #
    # cache_key = f"{termo}_{localizacao}_{pagina}"
    # cached_data = cache.get(cache_key)
    #
    # if cached_data:
    #     return {"source": "cache", "data": eval(cached_data)}
    #
    # payload = {
    #     "keywords": termo,
    #     "location": localizacao,
    #     "page": pagina
    # }
    # headers = {"Content-Type": "application/json"}
    #
    # async with httpx.AsyncClient(timeout=10) as client:
    #     try:
    #         response = await client.post(f"{JOOBLE_API_URL}{JOOBLE_API_KEY}", json=payload, headers=headers)
    #         response.raise_for_status()  # Lan√ßa erro caso a API retorne status != 200
    #         data = response.json()
    #     except httpx.HTTPStatusError as e:
    #         return {"error": f"Erro na API Jooble: {e.response.status_code}"}
    #     except httpx.RequestError:
    #         return {"error": "Erro de conex√£o com a API Jooble"}
    #
    # novas_vagas = data.get("jobs", [])[:15]
    # if not novas_vagas:
    #     return {"error": "Nenhuma vaga encontrada para esta p√°gina."}
    #
    # vagas_jooble = [
    #     {
    #         "titulo": vaga.get("title", "Sem t√≠tulo"),
    #         "empresa": vaga.get("company", "Empresa n√£o informada"),
    #         "localizacao": vaga.get("location", "Local n√£o informado"),
    #         "salario": vaga.get("salary", "Sal√°rio n√£o informado"),
    #         "data_atualizacao": vaga.get("updated", "Data n√£o informada"),
    #         "link": vaga.get("link", "#"),
    #         "descricao": vaga.get("snippet", "Descri√ß√£o n√£o dispon√≠vel")
    #     }
    #     for vaga in novas_vagas
    # ]
    #
    # # Salva no cache por 1 hora
    # cache.set(cache_key, str(vagas_jooble), ex=3600)
    #
    # return {"source": "live", "data": vagas_jooble}

    # =========================================================================
    #                       (AGORA USANDO jobspy)
    # =========================================================================
    # Aqui configuramos a busca do jobspy:
    # - site_name: lista de plataformas.
    # - search_term: pode usar o termo recebido por par√¢metro.
    # - google_search_term: ajustado dinamicamente conforme a cidade.
    # - location: usa a localiza√ß√£o recebida ou um default.
    # - results_wanted: quantas vagas no total (ajuste se quiser mais).
    # - hours_old: vagas postadas nas √∫ltimas X horas (72 horas no exemplo).
    # - country_indeed: 'Brazil' ou outro caso precise.

    # A lib jobspy n√£o lida diretamente com pagina√ß√£o igual a Jooble.
    # Ent√£o, por ora, estamos ignorando o par√¢metro 'pagina'.
    # Se quiser fazer algo como slicing manual para simular pagina√ß√£o,
    # voc√™ pode ajustar conforme necess√°rio.

    jobs = scrape_jobs(
        site_name=["indeed", "linkedin", "zip_recruiter", "glassdoor", "google", "bayt"],
        search_term=termo if termo else "desenvolvedor Python",
        google_search_term=f"{termo} jobs near {localizacao}" if termo and localizacao else "Python developer jobs near S√£o Paulo, SP",
        location=localizacao if localizacao else "S√£o Paulo, SP",
        results_wanted=15,  # Ajustado para 15, mantendo padr√£o similar ao Jooble
        hours_old=72,
        country_indeed='Brazil',
    )

    # Se quiser, pode salvar em CSV (opcional):
    # jobs.to_csv("vagas.csv", quoting=csv.QUOTE_NONNUMERIC, escapechar="\\", index=False)

    # Agora convertemos o DataFrame para a mesma estrutura esperada pelo front
    vagas = []
    for _, row in jobs.iterrows():
        vagas.append({
            "titulo": row.get("title", "Sem t√≠tulo"),
            "empresa": row.get("company_name", "Empresa n√£o informada"),
            "localizacao": row.get("location", "Local n√£o informado"),
            "salario": row.get("salary", "Sal√°rio n√£o informado"),
            "data_atualizacao": row.get("date", "Data n√£o informada"),
            "link": row.get("job_link", "#"),
            "descricao": row.get("snippet") or row.get("summary", "Descri√ß√£o n√£o dispon√≠vel")
        })

    if not vagas:
        return {"error": "Nenhuma vaga encontrada."}

    return {"source": "live", "data": vagas}
